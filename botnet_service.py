# Äáº£m báº£o táº¥t cáº£ cÃ¡c endpoint Ä‘á»u náº±m á»Ÿ lá» ngoÃ i cÃ¹ng

from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse

app = FastAPI()

_botnet_service = None

@app.post("/api/harvest-google")
async def api_harvest_google(request: Request):
    data = await request.json()
    dork_file = data.get("dork_file", "inputDork/general_search_dorks.txt")
    output_file = "outputDork/shops_fresh_2025_google.txt"
    dork_count = int(data.get("dork_count", 50))
    thread_count = int(data.get("thread_count", 10))
    use_proxies = data.get("use_proxies", False)
    use_ua_rotation = data.get("use_ua_rotation", False)
    min_delay = int(data.get("min_delay", 15))
    max_delay = int(data.get("max_delay", 45))
    # Äá»c dork tá»« file
    with open(dork_file, "r", encoding="utf-8") as f:
        dorks = [line.strip() for line in f if line.strip()][:dork_count]
    # Giáº£ láº­p search Google vÃ  ghi ra file output
    with open(output_file, "w", encoding="utf-8") as f_out:
        for dork in dorks:
            # Thay báº±ng logic search thá»±c táº¿
            f_out.write(f"https://google.com/search?q={dork}\n")
    return JSONResponse(content={"success": True, "message": f"Harvested {len(dorks)} dorks for Google."})

@app.post("/api/harvest-bing")
async def api_harvest_bing(request: Request):
    data = await request.json()
    dork_file = data.get("dork_file", "inputDork/general_search_dorks.txt")
    output_file = "outputDork/shops_fresh_2025_bing.txt"
    dork_count = int(data.get("dork_count", 50))
    thread_count = int(data.get("thread_count", 10))
    use_proxies = data.get("use_proxies", False)
    use_ua_rotation = data.get("use_ua_rotation", False)
    min_delay = int(data.get("min_delay", 15))
    max_delay = int(data.get("max_delay", 45))
    # Äá»c dork tá»« file
    with open(dork_file, "r", encoding="utf-8") as f:
        dorks = [line.strip() for line in f if line.strip()][:dork_count]
    # Giáº£ láº­p search Bing vÃ  ghi ra file output
    with open(output_file, "w", encoding="utf-8") as f_out:
        for dork in dorks:
            # Thay báº±ng logic search thá»±c táº¿
            f_out.write(f"https://bing.com/search?q={dork}\n")
    return JSONResponse(content={"success": True, "message": f"Harvested {len(dorks)} dorks for Bing."})

@app.post("/api/harvest-yandex")
async def api_harvest_yandex(request: Request):
    data = await request.json()
    dork_file = data.get("dork_file", "inputDork/general_search_dorks.txt")
    output_file = "outputDork/shops_fresh_2025_yandex.txt"
    dork_count = int(data.get("dork_count", 50))
    thread_count = int(data.get("thread_count", 10))
    use_proxies = data.get("use_proxies", False)
    use_ua_rotation = data.get("use_ua_rotation", False)
    min_delay = int(data.get("min_delay", 15))
    max_delay = int(data.get("max_delay", 45))
    # Äá»c dork tá»« file
    with open(dork_file, "r", encoding="utf-8") as f:
        dorks = [line.strip() for line in f if line.strip()][:dork_count]
    # Giáº£ láº­p search Yandex vÃ  ghi ra file output
    with open(output_file, "w", encoding="utf-8") as f_out:
        for dork in dorks:
            # Thay báº±ng logic search thá»±c táº¿
            f_out.write(f"https://yandex.com/search/?text={dork}\n")
    return JSONResponse(content={"success": True, "message": f"Harvested {len(dorks)} dorks for Yandex."})

@app.post("/api/harvest-duckduckgo")
async def api_harvest_duckduckgo(request: Request):
    data = await request.json()
    dork_file = data.get("dork_file", "inputDork/general_search_dorks.txt")
    output_file = "outputDork/shops_fresh_2025_duckduckgo.txt"
    dork_count = int(data.get("dork_count", 50))
    thread_count = int(data.get("thread_count", 10))
    use_proxies = data.get("use_proxies", False)
    use_ua_rotation = data.get("use_ua_rotation", False)
    min_delay = int(data.get("min_delay", 15))
    max_delay = int(data.get("max_delay", 45))
    # Äá»c dork tá»« file
    with open(dork_file, "r", encoding="utf-8") as f:
        dorks = [line.strip() for line in f if line.strip()][:dork_count]
    # Giáº£ láº­p search DuckDuckGo vÃ  ghi ra file output
    with open(output_file, "w", encoding="utf-8") as f_out:
        for dork in dorks:
            # Thay báº±ng logic search thá»±c táº¿
            f_out.write(f"https://duckduckgo.com/?q={dork}\n")
    return JSONResponse(content={"success": True, "message": f"Harvested {len(dorks)} dorks for DuckDuckGo."})

@app.post("/api/harvest-commoncrawl")
async def api_harvest_commoncrawl(request: Request):
    data = await request.json()
    max_files = int(data.get("max_files", 10))
    threads = int(data.get("threads", 5))  # ChÆ°a dÃ¹ng Ä‘a luá»“ng, Ä‘á»ƒ má»Ÿ rá»™ng sau
    
    try:
        loop = asyncio.get_event_loop()
        await loop.run_in_executor(None, harvest_common_crawl, max_files)
        return JSONResponse(content={"success": True, "message": f"Harvested {max_files} WARC files."})
    except KeyboardInterrupt:
        print("\n[CC] Harvesting interrupted by user")
        return JSONResponse(content={"success": False, "message": "Harvesting interrupted"}, status_code=500)
    except Exception as e:
        print(f"[ERROR] Harvesting failed: {e}")
        return JSONResponse(content={"success": False, "message": str(e)}, status_code=500)
# --- Common Crawl Harvest (NO WARC SAVE) ---
import requests, gzip, time

def harvest_common_crawl(max_files=30, keyword_file="inputDork/common_crawl_dorks.txt", output_file="outputDork/shops_fresh_2025_common_crawl.txt"):
    """
    Stream WARC files from Common Crawl, filter by keywords, and save matching URLs.
    Args:
        max_files (int): Number of WARC files to stream.
        keyword_file (str): Path to keyword file.
        output_file (str): Path to output file.
    """
    print("\n" + "="*80)
    print("STARTING COMMON CRAWL HARVEST")
    print("="*80 + "\n")
    
    index_url = "https://data.commoncrawl.org/crawl-data/index.html"
    html_result = {}

    # Fetch HTML using Scrapy
    import scrapy
    from scrapy.crawler import CrawlerProcess

    class CrawlIndexSpider(scrapy.Spider):
        name = "crawl_index"
        start_urls = [index_url]
        
        def parse(self, response):
            html_result['content'] = response.text
            html_result['status'] = response.status
            html_result['url'] = response.url
            print(f"\n[SCRAPY] Response Status: {response.status}")
            print(f"[SCRAPY] Response URL: {response.url}")
            print(f"[SCRAPY] HTML Length: {len(response.text)} characters\n")

    # Configure Scrapy
    process = CrawlerProcess(settings={
        "LOG_ENABLED": False,
        "USER_AGENT": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    })
    
    process.crawl(CrawlIndexSpider)
    print("\n[CC] Starting Scrapy crawler...")
    process.start()
    
    # Process results
    print("\n" + "="*80)
    print("SCRAPY FETCH RESULTS")
    print("="*80)
    
    if html_result.get('content'):
        html_content = html_result['content']
        print(f"\n[SUCCESS] Fetched HTML successfully!")
        print(f"[INFO] Status Code: {html_result.get('status')}")
        print(f"[INFO] URL: {html_result.get('url')}")
        print(f"[INFO] Content Length: {len(html_content)} characters")
        
        print("\n" + "="*80)
        print("HTML CONTENT PREVIEW (First 2000 characters)")
        print("="*80 + "\n")
        print(html_content[:2000])
        
        print("\n" + "="*80)
        print("HTML CONTENT PREVIEW (Last 1000 characters)")
        print("="*80 + "\n")
        print(html_content[-1000:])
        
        
    else:
        print("\n[ERROR] Failed to fetch HTML content!")
    
    print("\n" + "="*80)
    print("HARVEST COMPLETED")
    print("="*80 + "\n")
import json
import httpx
"""
ðŸ¤– Botnet Service
High-performance async botnet operations for KJC API testing
"""

from typing import Dict
from fastapi import HTTPException
import httpx
import asyncio
import time
import random


async def get_user_by_username_async(username: str) -> Dict:
    """Get user by username - Direct database access to avoid circular import"""
    try:
        import os
        from pymongo import MongoClient
        from bson import ObjectId
        
        # Direct database access to avoid circular import
        mongodb_url = os.getenv("MONGODB_URL")
        db_name = os.getenv("MONGODB_DATABASE", "kjc-group-staging")
        client = MongoClient(mongodb_url, serverSelectionTimeoutMS=5000)
        db = client[db_name]
        
        user = db.users.find_one({"username": username, "deletedAt": {"$in": [None, ""]}})
        if not user:
            client.close()
            return None
            
        # Convert ObjectId to string
        if "_id" in user:
            user["_id"] = str(user["_id"])
        for k, v in list(user.items()):
            if isinstance(v, ObjectId):
                user[k] = str(v)
                
        client.close()

        return user
    except Exception as e:
        print(f"Error getting user {username}: {str(e)}")
        return None


class BotBrowser:
    
    async def close_browser(self):
        """Close browser instance (cleanup resources including WebSocket)"""
        # Close WebSocket connection - AVOID accessing .open property (causes Code 1005)
        if self.websocket:
            try:
                # Just try to close without checking state - let exceptions handle closed connections
                await self.websocket.close()
                print(f"ðŸ”— Bot {self.username}: WebSocket connection closed")
            except Exception as e:
                print(f"ðŸ”— Bot {self.username}: WebSocket close error (expected): {e}")
            except Exception as e:
                print(f"âš ï¸ Bot {self.username}: Error closing WebSocket: {e}")
        
        # Cancel WebSocket monitoring task
        if self.websocket_task and not self.websocket_task.done():
            self.websocket_task.cancel()
            try:
                await self.websocket_task
            except asyncio.CancelledError:
                print(f"ðŸ”„ Bot {self.username}: WebSocket monitoring task cancelled")
            except Exception as e:
                print(f"âš ï¸ Bot {self.username}: Error cancelling WebSocket task: {e}")
        
        # Close HTTP client
        await self.http_client.aclose()
        print(f"ðŸ”´ Closed browser for bot: {self.username}")
    
    def __del__(self):
        """Destructor to ensure cleanup"""
        try:
            if hasattr(self, 'http_client') and not self.http_client.is_closed:
                asyncio.create_task(self.http_client.aclose())
        except:
            pass


class BotnetService:
    """
    ðŸš€ Optimized Botnet Service with async HTTP client
    Supports high-concurrency operations (up to 5000+ bots)
    """
    
    def __init__(self):
        """Initialize BotnetService with SJC scraper service"""
        self.active_bots = {}
    
    async def get_active_browsers(self) -> Dict:
        """Get information about all active browser instances with 5k scale analysis + caching"""
        import time
        
        # Cache browser info for 2 seconds to reduce get_session_info() calls 
        # This prevents race conditions with Socket.IO ping/pong
        current_time = time.time()
        cache_key = "active_browsers_cache"
        cache_timeout = 2.0  # seconds
        
        # Check if we have valid cached data
        if hasattr(self, '_browser_cache') and hasattr(self, '_browser_cache_time'):
            if current_time - self._browser_cache_time < cache_timeout:
                print(f"ðŸ”„ Using cached browser info ({len(self._browser_cache['browsers'])} bots)")
                return self._browser_cache
        
        # Generate fresh browser info (expensive operation)
        print(f"ðŸ”„ Refreshing browser info for {len(self.active_bots)} bots...")
        browsers_info = []
        
        for username, bot_browser in self.active_bots.items():
            session_info = await bot_browser.get_session_info()
            browsers_info.append(session_info)
        
        # Calculate 5k scale metrics
        current_browsers = len(self.active_bots)
        estimated_memory = current_browsers * 5  # 5MB per bot
        
        result = {
            "total_browsers": current_browsers,
            "browsers": browsers_info,
            "scale_analysis_5k": {
                "current_memory_usage_mb": estimated_memory,
                "max_concurrent_with_current_semaphore": 500,
                "estimated_5k_completion_time_seconds": 50,
                "no_io_blocking": True,
                "bottleneck": "semaphore_queue_management",
                "recommended_ram_for_5k": "25GB",
                "performance_rating": "excellent_for_5k_scale"
            }
        }
        
        # Cache the result
        self._browser_cache = result
        self._browser_cache_time = current_time
        
        return result
    
    async def close_bot_browser(self, username: str) -> Dict:
        """Close a specific bot's browser instance"""
        if username in self.active_bots:
            await self.active_bots[username].close_browser()
            del self.active_bots[username]
            return {
                "success": True,
                "message": f"Closed browser for bot: {username}",
                "remaining_browsers": len(self.active_bots)
            }
        else:
            return {
                "success": False,
                "message": f"No active browser found for bot: {username}"
            }
    
    async def close_all_browsers(self):
        """Close all active browser instances"""
        print(f"ðŸ”´ Closing {len(self.active_bots)} active browsers...")
        
        for username, bot_browser in self.active_bots.items():
            await bot_browser.close_browser()
        
        self.active_bots.clear()
        print("âœ… All browsers closed successfully")
    
    async def scrape_sjc(self, csv_data=None) -> Dict:
        """Filter card data from CSV"""
        return await self.sjc_service.scrape_sjc(csv_data=csv_data)
    
    async def close(self):
        """Close all browser instances when service shuts down"""
        await self.close_all_browsers()

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()



# --- FastAPI endpoint for /api/scrape-sjc ---
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse

_botnet_service = None

def get_botnet_service() -> BotnetService:
    global _botnet_service
    if _botnet_service is None:
        _botnet_service = BotnetService()
    return _botnet_service

@app.post("/api/scrape-sjc")
async def api_scrape_sjc(request: Request):
    service = get_botnet_service()
    
    # Get CSV data from request body if provided
    csv_data = None
    try:
        body = await request.json()
        csv_data = body.get('csv_data', None)
    except:
        pass
    
    # Run scrape_sjc with CSV data
    result = await service.scrape_sjc(csv_data=csv_data)
    return JSONResponse(content=result)